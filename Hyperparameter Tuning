# -*- coding: utf-8 -*-
"""
Created on Wed Oct 12 10:22:24 2022
Crosscorrelation to find best hyperparameters
@author: mousamoradi
"""


import tensorflow as tf
import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.layers import Dropout

from keras.callbacks import LearningRateScheduler

import numpy as np
from matplotlib import pyplot as plt

from sklearn.model_selection import GridSearchCV
from PIL import Image
import cv2
import matplotlib.pyplot as plt
#from keras.preprocessing.image import ImageDataGenerator
import glob
import os
import sys
import random
import numpy as np


np.random.seed(42)
from scipy import stats
from sklearn.preprocessing import LabelEncoder
from tensorflow.python.keras import regularizers

#plt.style.use("dark_background")


# fix random seed for reproducibility
np.random.seed(42)

# Loading patches
# convert to numpay array.....
dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  
label = []  #Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.

G1='B:/Macular cube data/Mousa_python/noCLAHE/dataset/256_patches/G1/images_with_useful_info/masks/'
G2='B:/Macular cube data/Mousa_python/noCLAHE/dataset/256_patches/G2/images_with_useful_info/masks/'
G3='B:/Macular cube data/Mousa_python/noCLAHE/dataset/256_patches/G3/images_with _useful_info/masks/'

#To avoid memory leakage we used 2000 samples
images = os.listdir(G1) 
images=images[0:2000]
for i, image_name in enumerate(images):    #Remember enumerate method adds a counter and returns the enumerate object
    if (image_name.split('.')[1] != 'db'):
        image = cv2.imread(G1+image_name)
        image = Image.fromarray(image, 'RGB')
        dataset.append(np.array(image))
        label.append(0)
        
images = os.listdir(G2)
images=images[0:2000]

for i, image_name in enumerate(images):    
    if (image_name.split('.')[1] != 'db'):
        image = cv2.imread(G2+image_name)
        image = Image.fromarray(image, 'RGB')
        dataset.append(np.array(image))
        label.append(1)
        
images = os.listdir(G3)
images=images[0:2000]

for i, image_name in enumerate(images):    
    if (image_name.split('.')[1] != 'db'):
        image = cv2.imread(G3+image_name)
        image = Image.fromarray(image, 'RGB')
        dataset.append(np.array(image))
        label.append(2)
        
dataset = np.array(dataset)
label = np.array(label)
print("Dataset size is ", dataset.shape)
print("Label size is ", label.shape)        



from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.20, random_state = 42)
# reshape images to 1D so we can just work with dense layers
#For this demo purposes
#x_train = x_train.reshape(47998, 256)
#x_test = x_test.reshape(10000, 256)

num_classes = 3

# One hot encoding for categorical labels
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

#Take a subset of train for grid search. Let us take 10% for now
from sklearn.model_selection import train_test_split
x_grid, x_not_use, y_grid, y_not_use = train_test_split(x_train, y_train, test_size=0.9, random_state=42)

# build the model
input_dim = x_grid.shape[1]




#NOTE: Add default optimizer, otherwise throws error 'optimizer not legal parameter'
def define_model(activation='relu', init_weights='uniform', optimizer='Adam'):   
    model = Sequential()
    model.add(Dense(64, activation=activation, kernel_initializer=init_weights, 
                    input_dim = input_dim)) 
    model.add(Dropout(0.1))
    model.add(Dense(64, kernel_initializer=init_weights, activation=activation))
    model.add(Dense(num_classes, kernel_initializer=init_weights, activation='softmax'))
    
    # compile the model
    model.compile(loss='categorical_crossentropy',
                  optimizer=optimizer,      
                  metrics=['acc'])
    return model


# implement the Scikit-Learn classifier interface
# requires model defined as a function, which we already have
from keras.wrappers.scikit_learn import KerasClassifier
batch_size = 32
epochs = 100

model = KerasClassifier(build_fn=define_model, 
                        epochs=epochs, 
                        batch_size = batch_size, 
                        verbose=1)


activation = ['softmax', 'relu', 'sigmoid']
#Also try softplus, tanh, linear, hard_sigmoid 
init_weights = ['uniform', 'normal', 'he_uniform']
#Also try lecun_uniform, he_normal, glorot_normal, etc. 
optimizer = ['SGD', 'RMSprop', 'Adam']

param_grid = dict(activation=activation, init_weights=init_weights, optimizer=optimizer)



# -1 refers to using all available CPUs
#Cross validation, cv=5
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=16, cv=5)

grid_result = grid.fit(x_grid, y_grid)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("Mean = %f (std=%f) with: %r" % (mean, stdev, param))
